# -*- coding: utf-8 -*-
"""FER_2013.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QcwlgjCaI42MwUqeezVW2w6OKv_R3QC0
"""

# ุชุซุจูุช ุงูู kaggle (ูู ูู ููู ูุซุจุชุงู) ูุชุฃูุฏ ูู ูุฌูุฏ ุตูุงุญูุงุช
!pip install -q kaggle

from google.colab import files
uploaded = files.upload()

!mv "/content/kaggle (1).json" /content/kaggle.json

from pathlib import Path
import os

kaggle_json_path = Path('/content/kaggle.json')

if kaggle_json_path.exists():
    os.makedirs('/root/.kaggle', exist_ok=True)
    !cp /content/kaggle.json /root/.kaggle/
    !chmod 600 /root/.kaggle/kaggle.json
    print('โ kaggle.json moved to /root/.kaggle and permissions set.')
else:
    print('โ ูู ุฃุฌุฏ kaggle.json ูู /content โ ุงุฑูุน ููู kaggle.json ุซู ุฃุนุฏ ุชุดุบูู ูุฐู ุงูุฎููุฉ.')

# ุชูุฒูู ูุฌููุนุฉ ุจูุงูุงุช FER-2013 ูู Kaggle

!kaggle datasets download -d msambare/fer2013 -q --unzip

"""ูุง ูุงุฆุฏุฉ ุงููููุฏ (ImageDataGenerator)ุ

ุงูููุฑุฉ ุงูุฃุณุงุณูุฉ ๐
ุจุฏููุง ูู ุชุญููู ูู ุงูุตูุฑ ุฅูู ุงูุฐุงูุฑุฉ ุฏูุนุฉ ูุงุญุฏุฉ (ููู ูุณุชุญูู ุฃุญูุงููุง ุนูุฏูุง ุชููู ุขูุงู ุงูุตูุฑ)ุ ูููู ุงููููุฏ ุจุชุญููู ุฏูุนุงุช ุตุบูุฑุฉ (batches) ูู ุงูุตูุฑ ุฃุซูุงุก ุงูุชุฏุฑูุจ ููุท.

๐น ุฃู ุฃูู:

ููุฑุฃ ุงูุตูุฑ ูู ุงููุฌูุฏ ูุจุงุดุฑุฉ.

ูุทุจูู ุนูููุง ุงูุชุญุณููุงุช ุงููุทููุจุฉ (ูุซู ุงูุชุฏููุฑุ ุงููุตุ ุงูุชุทุจูุน).

ูุฑุณููุง ุฅูู ุงููููุฐุฌ ุนูู ุดูู ุฏูุนุงุช (batch) ุจุดูู ูุณุชูุฑ ุฃุซูุงุก ุงูุชุฏุฑูุจ.

ููุฐุง ูุง ููุณููู ุจู ุงูุชุบุฐูุฉ ุงููุณููุฉ (lazy loading) ุฃู ุงูุชุบุฐูุฉ ุฃุซูุงุก ุงูุชุฏุฑูุจ.
"""

import os
import tensorflow as tf

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns


#ุฅูุดุงุก ูููุฏ (generator) ูููู ุจูุฑุงุกุฉ ุงูุตูุฑ ูู ุงููุฌูุฏุงุช ูุน ุฅููุงููุฉ ุชุญุณูููุง ุฃู ุชุนุฏูููุง (augmentation) ุฃุซูุงุก ุงูุชุฏุฑูุจ.
IMG_SIZE = 48 #ุญุฌู ุงูุตูุฑุฉ ุงููุทููุจ ุนูุฏ ุฅุฏุฎุงููุง ูููููุฐุฌ (ูู ุตูุฑุฉ ุณูุชู ุชุญููููุง ุฅูู 48ร48 ุจูุณู).
BATCH_SIZE = 32 #ุนุฏุฏ ุงูุตูุฑ ุงูุชู ุณุชูุนุงูุฌ ุฏูุนุฉ ูุงุญุฏุฉ ูู ูู ุฎุทูุฉ ุชุฏุฑูุจ (batch size = 32).

# ุฅูุดุงุก ูููุฏ ููุตูุฑ ูุน ุงูุชูุณูุน augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255, #ุชุทุจูุน ุงูุตูุฑ ุจุชุญููู ุงูููู ูู [0,255] ุฅูู [0,1] ูุชุณุฑูุน ุงูุชุนูู.
    rotation_range=10, #ุชุฏููุฑ ุงูุตูุฑุฉ ุนุดูุงุฆููุง ุจุฒุงููุฉ ุชุตู ุฅูู ยฑ10 ุฏุฑุฌุงุช.
    width_shift_range=0.1, #ุชุญุฑูู ุงูุตูุฑุฉ ุฃููููุง ุจูุณุจุฉ 10% ูู ุนุฑุถูุง.
    height_shift_range=0.1,#ุชุญุฑูู ุงูุตูุฑุฉ ุนููุฏููุง ุจูุณุจุฉ 10%.
    horizontal_flip=True #ููุจ ุงูุตูุฑุฉ ุฃููููุง ุนุดูุงุฆููุง.

  #ุงููุฏู: ุฌุนู ุงููููุฐุฌ ูุฑู ุชููุนูุง ุฃูุจุฑ ูู ููุณ ุงูุจูุงูุงุช (ุฒูุงุฏุฉ ุงููุฏุฑุฉ ุนูู ุงูุชุนููู).
)


'''
๐น ููุง ูููุฏ ุจูุงูุงุช ููุงุฎุชุจุงุฑ ููุทุ
ููู ุจุฏูู ุฃู ุชุนุฏูู (augmentation) ูุฃููุง ูุฑูุฏ ุชูููู ุงููููุฐุฌ ุนูู ุงูุตูุฑ ุงูุฃุตููุฉ ููุท.
ููุชูู ุจุชุทุจูุน ุงูููู [0,1].
'''
test_datagen = ImageDataGenerator(rescale=1./255)


#ูุฐุง ุงูุณุทุฑ ููุดุฆ ูููุฏ ูุนูู ููุตูุฑ ุงูุชุฏุฑูุจูุฉ ููุฑุฃ ุงูุตูุฑ ูู ุงููุฌูุฏ training/.
# ุชุญููู ุงูุจูุงูุงุช ูู ุงููุฌูุฏุงุช
train_generator = train_datagen.flow_from_directory(
    '/content/train',          # ูุณุงุฑ ูุฌูุฏ ุงูุชุฏุฑูุจ
    target_size=(IMG_SIZE, IMG_SIZE), #ูุบูุฑ ุญุฌู ูู ุตูุฑุฉ ุฅูู 48ร48 ุจูุณู.
    color_mode='grayscale',  # ูุฃู ุตูุฑ FER-2013 ุฃุจูุถ ูุฃุณูุฏ
    batch_size=BATCH_SIZE, #ูุนูุฏ 32 ุตูุฑุฉ ูู ูู ุฏูุนุฉ ุชุฏุฑูุจ.
    class_mode='categorical'
    #ูุนูู ุฃู ุงูุชุตูููุงุช ุณุชููู one-hot encoded (ูุซู [0,0,1,0,0,0,0] ูุชุตููู ุงููุฌู).
)

#the same train data
test_generator = test_datagen.flow_from_directory(
    '/content/test',             # ูุณุงุฑ ูุฌูุฏ ุงูุงุฎุชุจุงุฑ
    target_size=(IMG_SIZE, IMG_SIZE),
    color_mode='grayscale',
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False        # ููุญูุงุธ ุนูู ุชุฑุชูุจ ุงูุตูุฑ ูู ุงูุชูููู

 )

  #shuffle=False: ูุนูู ูุง ูุฎูุท ุชุฑุชูุจ ุงูุตูุฑ
 # (ุญุชู ูุญุงูุธ ุนูู ุชุฑุชูุจูุง ุงูุฃุตูู ุนูุฏ ุญุณุงุจ ุงููุชุงุฆุฌ ุฃู ุงูููุงุฑูุฉ ูุน ุงูุชุณููุงุช ุงูุญููููุฉ).


EPOCHS = 25

#ููุง ูุญุตู ุนูู ุนุฏุฏ ุงููุฆุงุช (classes) ุชููุงุฆููุง ูู ุงููููุฏ train_generator.
#train_generator.class_indices โ ูุงููุณ ูุฑุจุท ุงุณู ูู ูุฆุฉ ุจุฑูู.
#len(...) โ ุนุฏุฏ ุงููุฆุงุช ุงูุฅุฌูุงูู.
#ูุทุจุนู ูุชุฃููุฏ ุนุฏุฏ ุฃุตูุงู ุงููุฌู ุงูููุฌูุฏุฉ ูู  dataset
num_classes = len(train_generator.class_indices)
print("ุนุฏุฏ ุงููุฆุงุช:", num_classes)

# ===== ุจูุงุก ูููุฐุฌ CNN ุจุณูุท =====
input_shape = (IMG_SIZE, IMG_SIZE, 1)#ุชุญุฏูุฏ ุดูู ุงูุตูุฑุฉ ุงูุฏุงุฎูุฉ ูููููุฐุฌ (ุงูุนุฑุถ  ุงูุฅุฑุชูุงุน ููุงุช ุงูููู )

model = models.Sequential([
    layers.Input(shape=input_shape),#ูุญุฏุฏ ุดูู ุงููุฏุฎูุงุช.

    layers.Conv2D(32, (3,3), activation='relu', padding='same'),
    #padding='same' โ ูุถูู ุจูุงุก ุฃุจุนุงุฏ ุงูุตูุฑุฉ ุจุนุฏ ุงูุงูุชูุงู ููุณูุง.
    layers.BatchNormalization(),
    #ูุญุณู ุณุฑุนุฉ ุงูุชุฏุฑูุจ ููููู ุงูู overfitting ุนู ุทุฑูู ุชูุญูุฏ ุชูุฒูุน ุงูููู ุฏุงุฎู ูู batch.
    layers.MaxPooling2D((2,2)),
    #ุชูููู ุฃุจุนุงุฏ ุงูุตูุฑุฉ (downsampling) ุฅูู ุงููุตู.
    #ูุญุงูุธ ุนูู ุฃูู ุงูููุฒุงุช ุงููููุฒุฉ.

    #ููุณ ุงูููุฑุฉ ุงูุณุงุจูุฉ ููู ุนุฏุฏ ุงูููุงุชุฑ ุฒุงุฏ ุฅูู 64 โ ุชุนูู ููุฒุงุช ุฃูุซุฑ ุชุนููุฏูุง.
    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2,2)),
    #Dropout(0.25) โ ุฅููุงู 25% ูู ุงูุฎูุงูุง ุนุดูุงุฆููุง ุฃุซูุงุก ุงูุชุฏุฑูุจ ูุชูููู ุงูู overfitting.
    layers.Dropout(0.25),

    #ุนุฏุฏ ุงูููุงุชุฑ ุฃุตุจุญ 128 โ ุชูุซูู ููุฒุงุช ุฃูุซุฑ ุชุนููุฏูุง.
    #ููุณ ุฎุทูุงุช Normalization, Pooling, Dropout.
    layers.Conv2D(128, (3,3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),

    #Flatten() โ ุชุญููู ุงููุฎุฑุฌุงุช ุซูุงุฆูุฉ ุงูุฃุจุนุงุฏ ูู Conv ุฅูู ูุชุฌู 1D ููุชู ุฅุฏุฎุงููุง ููู Dense.
    layers.Flatten(),
    #Dense(128, activation='relu') โ ุทุจูุฉ fully connected ุจูุง 128 ุฎููุฉ ูุชุฌููุน ุงูููุฒุงุช.
    layers.Dense(128, activation='relu'),
    #BatchNormalization() + Dropout(0.5) โ ุชูุญูุฏ ุงูููู ูุชูููู overfitting.
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    #Dense(num_classes, activation='softmax') โ ุทุจูุฉ ุงูุฅุฎุฑุงุฌ.
    layers.Dense(num_classes, activation='softmax')
])
#ุชุฌููุน ุงููููุฐุฌ
#optimizer='adam' โ ุฎูุงุฑุฒููุฉ ุชุญุณูู ุชููุงุฆูุฉ ูุนุงูุฉ.
#loss='categorical_crossentropy' โ ููุงุณุจุฉ ูุชุตููู ูุชุนุฏุฏ ุงููุฆุงุช.
#metrics=['accuracy'] โ ูุฑุงูุจ ุฏูุฉ ุงููููุฐุฌ ุฃุซูุงุก ุงูุชุฏุฑูุจ.
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ===== ุชุฏุฑูุจ ุงููููุฐุฌ =====
callbacks = [
    # ุชูููู ูุนุฏู ุงูุชุนูู ุฅุฐุง ุชูููุช ุงูุฎุณุงุฑุฉ ุนู ุงูุชุญุณู
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),
    # ุฅููุงู ูุจูุฑ ุนูุฏ ุนุฏู ุงูุชุญุณู
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1)
]

history = model.fit(
    train_generator,
    validation_data=test_generator,
    epochs=EPOCHS,
    callbacks=callbacks
)

# ===== ุฑุณู ุงูุฎุณุงุฑุฉ ูุงูุฏูุฉ =====
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.title('Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.title('Accuracy')
plt.legend()
plt.show()

# ===== ุชูููู ุงููููุฐุฌ =====
test_generator.reset()
y_pred_probs = model.predict(test_generator)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = test_generator.classes

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_true, y_pred, target_names=list(train_generator.class_indices.keys())))

# ===== ุญูุธ ุงููููุฐุฌ =====
model.save('fer2013_cnn_model.h5')
print('โ ุงููููุฐุฌ ุชู ุญูุธู ุจุงุณู fer2013_cnn_model.h5')